########################  HDFS Quick Preview #######################

HDFS: Hadoop Distributed File System. Google published its paper GFS and on the basis of that HDFS was developed. 
It states that the files will be broken into blocks and stored in nodes over the distributed architecture.

-HDFS replication factor is 3. It provides fault tolerance, reliability, and high availability.

-There are several datanodes in the cluster which store HDFS data in the local disk. Datanode sends a heartbeat message to namenode periodically to indicate that it is alive. Also, 
it replicates data to other datanode as per the replication factor.

-Distributed Storage - As HDFS stores data in a distributed manner. It divides the data into small pieces and stores it in different nodes of the cluster. In this manner, Hadoop 
Distributed File System  provides a way to map reduce to process a subset of large data, which 
is broken into smaller pieces and stored in multiple nodes, parallelly on several nodes. MapReduce is the heart of Hadoop but HDFS is the one which provides it all these capabilities.

*Blocks: A Block is the minimum amount of data that it can read or write.

-HDFS blocks are 128 MB by default and this is configurable.Files n HDFS are broken into block-sized chunks,
which are stored as independent units.Unlike a file system, if the file is in HDFS is smaller than block size, then it does not occupy full block?s size,
 i.e. 5 MB of file stored in HDFS of block size 128 MB takes 5MB of space only.The HDFS block size is large just to minimize the cost of seek.

-We (client and admin) do not have any control on the block like block location. Namenode decides all such things.

-Replication - 

- High Availability

-Data Reliability , fault tolerence

Hadoop Distributed File System (HDFS) – It is the storage layer of Hadoop.
Map-Reduce – It is the data processing layer of Hadoop.
YARN – It is the resource management layer of Hadoop.


* Name Node: HDFS works in master-worker pattern where the name node acts as master.
Name Node is controller and manager of HDFS as it knows the status and the metadata of all the files in HDFS; the metadata information being file permission, 
names and location of each block.The metadata are small, so it is stored in the memory of name node,allowing faster access to data. 
Moreover the HDFS cluster is accessed by multiple clients concurrently,so all this information is handled bya single machine. 

-The file system operations like opening, closing, renaming etc. are executed by it.

-Name node stores metadata like filename, the number of blocks, number of replicas, a location of blocks, block IDs etc. 


* Data Node: They store and retrieve blocks when they are told to; 
by client or name node. They report back to name node periodically, with list of blocks that they are storing.
 The data node being a commodity hardware also does the work of
-block creation, deletion and replication as stated by the name node.

* we need to go to the name node to check what is there in the cluster

*cd etc/hadoop/conf - you have all the properties witch control the environment of hdfs or yarn etc

*core-site.xml - it will have the details of your cluster mailnly it will have the uri of your name node (fs.defultFS)

*hdfs-site.xml - we can see the properties such as blocksize(it will divide the huge file into blocks. if the block size is 128 mb and you have 10 gb file it will divide into 
		80 small blocks). to get failover and fault tolerence of each of these blocks the replication is defult 3 so the 80 blocks will have 3 copies of them those
		3 copies are stored in multiple nodes on the cluster so, if some of the node go down .we still have the valid copie to run your applications.

*hadoop fs - command line interface (i will list bunch of commands)

* hadoop fs -help CopyFromLocal - it will explain you the command 

* -CopyFromLocal, -Put - used to copy the file from local to hdfs

*hdfs fsck /user/root/cards -files -blocks -locations - fsck (filesystem check) to check file system blocks and locations 

############################ YARN Quick Preview ###########################

*Yarn: Yet another Resource Negotiator is used for job scheduling and manage the cluster.

Yet Another Resource Manager takes programming to the next level beyond Java ,
 and makes it interactive to let another application Hbase, Spark etc. to work on it.Different Yarn applications can co-exist on the same cluster so MapReduce, 
Hbase, Spark all can run at the same time bringing great benefits for manageability and cluster utilization.

*Client: For submitting MapReduce jobs.
*Resource Manager: To manage the use of resources across the cluster
*Node Manager:For launching and monitoring the computer containers on machines in the cluster.
*Map Reduce Application Master: Checks tasks running the MapReduce job. 
The application master and the MapReduce tasks run in containers that are scheduled by the resource manager, and managed by the node managers.

*Map Reduce: This is a framework which helps Java programs to do the parallel computation on data using key value pair. 
The Map task takes input data and converts it into a data set which can be computed in Key value pair. 
The output of Map task is consumed by reduce task and then the out of reducer gives the desired result.

* yarn is the distributed file system frame work 

*cd etc/hadoop/conf - you have all the properties witch control the environment of hdfs or yarn etc

*Yarn-site.xml- it will have the uri for resourcemanager ( we need to check mainly memory total and vcores(executers) total)

*cd etc/spark/conf -> vi spark-env.sh - have the defult values of executer core and executer memory etc 

* du -s -h (path) - we can get the size of data set 

################################ Setup Data Sets ###############################

copyping data from local to hadoop file system

################################ Spark Introduction ############################

* Spark is noting but distributed processing engin and it provides bunch of api's to facilitate distributed computing.

###############################  Core Spark - Intialize the Job ######################

* spark-shell --master yarn \ 

--deploy-mode client \
--num-executors 1 \
--executer-memore 2048M  - it will lunch the spark in scala context in cluster mode (forward slash is used to break your code in multiple lines in linux context)

*spark-shell --help - we will get the list of control arguments 

*by using the tracking URL we can track the executing job . each time you launch new session in spark it will give new tracking URL.

* sc.stop - we can stop the spark session 

* import org.apache.spark.SparkContext , import org.apache.spark.SparkConf , 
val conf = new SparkConf().
setMaster("yarn-client").
setAppName("Daily Product Revenue") - in scala we can supperate long line of code by supperating after .
 val sc = new SparkContext(conf)

*:history - we can get the history from reple

################################## Resilient Distributed Data Sets (RDD)######################

* RDD is similar to collections witch contain n number of elements in it.

* large data sets are divided into partitions and persist in memory. while prosessing if any of the node goes down for what ever reason what ever partitios 
persist in that node will be rebuilt in the surviving nodes witch makes the RDD resilient. so it is fault tolerent.

*there are two ways to create RDD
1. by reading data from files ( from HDFS etc)
2.collection -parallize
 * by reading data from files ( from HDFS etc)
*val orders = sc.textFile("[file path]") - read data from the file (textFile is the api to read data from file if it has all alpha numeric values ).it will create Rdd out of it 

* orders.first - it will give the first record.to check weather the file is copied in correct path or not 
=
*collection-parallelize -> val l = (1 to 100).tolist

val r = sc.parallelize(l) - it is used to create RDD out of the collection 

*if we had a situation like if we want to read data from local file system 

1. read data from local using scala.io.source and create a collection out of it and then parallelize it.

import scala.io.source
val productList = source.fromFile(["individual file path in local file system including file name"]).getLines
val productsRDD = sc.parallelize(productsList.toList)

################################## Previewing the data ####################################

collect(),count(),first(),take(n) - this are the action used to preview the data 

Collect(),take(n) - it will convert RDD t0 singlr thereded array .please be very care full while using collect use only for small data sets 

foreach(func) - we cannot use this api directly on rdd. first we need to convert rdd to arry by using collect or take then we can use for each 

################################# Filtering the data ######################################

*Transformations - A Transformation is a function that produces new RDD from the existing RDDs but when we want to work with the actual dataset, at that point Action is performed.

EX:map(func), filter(func)

* orders.map(order => order.split(",")(3).take(10).foreach(println) - it will supperate the status coloumn from orders and form new RDD

################################# Accumulators #############################################

* code what ever using spark Api's wont get executed immideatly it will create only dag until we invoke any action

* local variables are accessible only to the driver program. we cant use the local variables as part of the spark api it wont work.so, accumulators came into picture

* val orderscompletedcount = sc.accumulator(0,"Orders Completed Count") - we cant increment the local variable in the spark api so we need to use the accumulators to increment the value

################################# Converting into key value pairs - map ###########################. 

* if we want to join two tables. first we want to conver it into key value pairs witch is tuple and create nested tuple of next table with one comman value 

*val ordersmap = ordersFilterd.map(order => {  - it will create new Rdd ordersmap of type tuple 
 (order.split(",")(0).toint, order.split(",")(1))
})

################################ Joining data sets ################################

* we can join both the datasets (key value pairs) by useing join API 

*val ordersJoin = ordersMap.join(orderItemMap) - it will join two data sets 

*val ordersLeftOuterJoin = ordersMap.LeftOuterJoin(orderItemMap) - iw will give all the data in left dataset and the assiciated data on the right data set.

################################ Get Daily Revenue Per Product - reduceByKey #################### 

* we can aggregate data by using - reduseByKey - better in performance as it uses combiner and easy to use ,
				 aggregateByKey - better in performance as it uses in bit complecate scenarios 
						 where combiner and reduser logic are different 
 				groupByKey  - poor in performance, should be given low priority 

*val ordersJoinMap = ordersJoin.map(rec => ((rec._2._1, rec._2._2._1), rec._2._2._2) - to eleminate the order_id witch is not reqiresd to compute daily revenue by product

* val dailyRevenuePerProductId =  ordersJoinMap.reduseByKey((total, revenue) => total + revenue) it will create new RDD

################################## Daily Revenue and Count Per Product - aggregateByKey ################################

aggregateByKey - better in performance as it uses in bit complecate scenarios 
						 where combiner and reduser logic are different

*val dailyReveneABK = ordersJoinMap.aggregateByKey((0.0, 0))(       -(0.0, 0) - 0.0 is revenue , 0 is for count of products each day ( we need the out put like that)
(i, revenue) => (i._1 + revenue, i._2 + 1),			     (i, revenue) => (i._1 + revenue, i._2 + 1) - i is noting but tuple (0.0, 0) , revenue is actual revenue of type float
(i1, i2) => (i1._1 + i2._1, i1._2 + i2._2 )				the out put of i(intermediate logic is the input for the next one)
) 
 
############################## Execution life cycle ##################################

* it will be complicated to join the another data set to get the product name so the solution is "Brodcast variables"

* Brodcar variables are used to simplify the development process and improves the performance 

* if we have executer memory 1 gb - we can run the broadcast variable up to 300 to 400 mb . it should not go beyond memory allocated - mempry reqired for other over head (IT WILL TAKE 25 TO 75% OF GIVEN MEMRY)

*when we use broadcast variable it actually cashed into memory in each and every task we can do the lookup in any stage we want 

############################## Broadcast Variables Implementation #############################

* Broadcast variable use - when we want to join very large data set to a small data set 

* how to use - create hash map of a data set and do a lookup. smaller data set will be available in each of the task which is processing larger data set .

* val Productsmap = Products.map(order => {  - if we want to create hash map out of trhe map we need give .tomap then it will be a hash map 
 (order.split(",")(0).toint, order.split(",")(1))
}).tomap

*Productsmap.get(productid) - we can do the lookup by orderid witch is the key in hash map .it will give product name of type some 

*Productsmap.get(Productid).get - .get is used to get the original type

* val bv = sc.broadcast(Productsmap) - we need to pass the hash map witch we need to broadcast

* dailyRevenuePerProductName = dailyRevenuePerProduct.map(rec => { -(bv.value will retrive the original value) it will do the lookup by using productId and retrive the product name of type some 
((rec._1._1, bv.value.get(rec._1._2)), rec._2)

}).take(100).foreach(println)

*dailyRevenuePerProductName.take(100).foreach(println)

############################### Sorting data ###################################

*to sort the data first we need to create composit key with date and revenu 

* then we can use sortByKey to sort the data

*dailyRevenuePerProductName.sortByKey().take(100).foreach(println) - to get the sorted date and revenue assending order, if we wnat to sort in decending order we need to just say false 

*we need to negate the revenue while creating composit key so when we use sortByKey it will sort date in asc and revenue in dec with - in frount of it.

*dailyRevenuePerProductName = dailyRevenuePerProduct.map(rec => { 
((rec._1._1, -rec._2),bv.value.get(rec._1._2).get )

*we need the comma seperated values so we need to concatinate 

*dailyRevenuePerProductName = dailyRevenuePerProduct.map(rec => {  - so the date is in asending order and revenue is in decending order with the actual record 
((rec._1._1, -rec._2),rec._1._1 +"," + rec._2 + ","+ bv.value.get(rec._1._2).get )

* val dailyRevenuePerProductNameSorted = dailyRevenuePerProductName.sortByKey() -

* we need to discard the key in dailyRevenuePerProductNameSorted RDD so we need to convert the composit key to string and priserve the value 

*val dailyRevenuePerProductNameSorted = dailyRevenuePerProductName.sortByKey().map(rec => rec._2)

* there is another method to do this insted of concatenating. we need to create tuple out of it 

*dailyRevenuePerProductName = dailyRevenuePerProduct.map(rec => {  - it is a tuple 
((rec._1._1, -rec._2),(rec._1._1 , rec._2 , bv.value.get(rec._1._2).get )

**val dailyRevenuePerProductNameSorted = dailyRevenuePerProductName.sortByKey().map(rec => rec._2.productIterator.mkString(",")) - tuple will have iteraror 
																it will convert the tuple into delimeterd string 
################################### Saving data to file system ################################

*dailyRevenuePerProductNameSorted .saveASTextFile("HDFS path ") - (saveASOjectFile) - to copy the data to HDFS

*sc.textFile("HDFS path ").take(100).foreach(println) - to preview the data

* hadoop fs -get /HDFS path /local path - to copy from hdfs to local file system - there will be no of files = no of tasks in this case there are 2 taks so there will be two files 